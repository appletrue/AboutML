## 凸优化

凸优化理论本身非常博大，对于广大仅仅想要了解一下机器学习或者深度学习来说，稍微了解一点凸优化也就够了。在实际工程问题中，比如深度神经网络的求解优化问题，都是非凸的，因此很多凸优化理论中非常有价值的定理和方法，在非凸优化问题中不适用，或者说并没有收敛保证等

**1. 为什么要学习凸优化？**

凸优化在数学规划领域具有非常重要的地位。一旦将一个实际问题表述为凸优化问题，大体上意味着相应问题已经得到彻底解决，这是非凸的优化问题所不具有的性质。其应用非常广泛，机器学习中很多优化问题都要通过凸优化来求解；在非凸优化中，凸优化同样起到重要的作用，很多非凸优化问题，可以转化为凸优化问题来解决；

**2. 什么是优化问题？** 

$min f_0(x),  s.t. f_i(x) <=0,i=1,2,...,m; h_i(x)=0,i =1,2,...,p $（1）

 表示在所有满足$及f_i(x) <=0,i=1,2,...,m;及 h_i(x)=0,i =1,2,...,p $ 的x 中找出使 $f_0(x)$最小的x。

这里，$x \in R^n$ 称为问题的优化变量，函数 $：f_0：R^n \rightarrow R$称为目标函数，不等式 $f_i(x)<=0$ 称为不等式约束，相应的$f_i:R^n \rightarrow R,i=1,2,...m$称为不等式约束函数，方程组h_i(x)=0称为等式约束，相应的$h_i:R^n \rightarrow R,i=1,2,...p$称为等式约束。

如果没有约束，即m=p=0,称问题（1）为无约束问题。 

对目标和所有约束函数有定义的点的集合，称为优化问题（1）的定义域。

3，什么是凸优化问题？

$min f_0(x),  s.t. f_i(x) <=0,i=1,2,...,m; a_i^Tx=b_i,i =1,2,...,p $

上述优化问题中，f_i(x),i=0,1,...m 是凸函数。此类优化问题称为凸优化问题。

对比优化问题（1），目标函数和不等式约束为凸函数，等式约束是仿射函数的优化问题属于凸优化问题。

4，什么是凸函数？

函数 $f:R^n \rightarrow R$定义域是凸集，并且对于$和\forall x,y \in dom f 和 \forall \theta ,0<=\theta<=1$ 有 $f(\theta x +(1-\theta)y) <= \theta f(x) + (1-\theta)f(y)$ 则称函数f 是凸的。

![20180417tu](https://github.com/appletrue/NoteML/blob/master/PICs/20180417tu.png)

凸函数的几何意义表示为函数任意两点的连线上的取值大于该点在函数上的取值。

5，凸函数的一阶充要条件

一阶条件的意义是，对于函数在定义域的任意取值，函数的值都大于或者等于对函数在这点的一阶近似。用图来说明就是下图

![20180417tu2](https://github.com/appletrue/NoteML/blob/master/PICs/20180417tu2.png)

就是说凸函数总是在其任意一点的切线的上方（或者可以有部分相切的）。通过图可以很清楚地理解这个充要条件，但是，具体在应用中，我们不可能对每一个点都去计算函数的一阶导数，因此后面会说道利用凸函数的二阶特征来进行判断一个函数是否是一个凸函数。

6，凸函数的二阶充要条件

记函数的一阶导数和二阶导数为g和H，

$g = \nabla f = \begin{bmatrix} \frac{\partial f }{\partial x_1} \\ \frac{\partial f }{\partial x_2} \\ \vdots\\\frac{\partial f }{\partial x_n} \\  \end{bmatrix} \ \ \ \ \quad  H = \nabla^2 f = \begin{bmatrix}  \frac{\partial^2 f }{\partial^2 x_1}& \frac{\partial^2 f }{\partial x_1 x_2} & \cdots  &\frac{\partial ^2 f }{\partial x_1x_n} \\ \frac{\partial^2 f }{\partial^2 x_1x_2}& \frac{\partial^2 f }{\partial  x_2^2} &\cdots  &\frac{\partial ^2 f }{\partial x_2x_n} \\ \vdots & \vdots  & \ddots  & \vdots \\ \frac{\partial^2 f }{\partial^2 x_nx_1}& \frac{\partial^2 f }{\partial x_n x_2} &\cdots &\frac{\partial ^2 f }{\partial x_n^2} \\ \end{bmatrix}$

其中要求函数ff二阶可微，则函数ff在定义域上式convex函数的充要条件是：函数的二阶导数（即Hessian矩阵）是半正定的，也就是所有的特征值都是大于等于0的。特殊的，对于一元函数，可以退化为f′′(x)≥0。

## Hessian矩阵与牛顿法

上面提到了函数的二阶导数是Hessian矩阵，Hessian矩阵经常用于牛顿法优化方法中。牛顿法是一种迭代求解方法，有一阶和二阶方法，主要应用在两个方面：1、求方程的根， 2、 最优化方法。

**（1）牛顿迭代求解方程的根：**

并不是所有的方程都有求根公式, 或者求根公式很复杂, 导致求解困难. 利用牛顿法, 可以迭代求解。原理是利用泰勒公式, 在x0处展开, 且展开到一阶, x=x0+δ

$f(x) = f({x_0}) + (x – {x_0})f’({x_0})$

求解方程f(x)=0，即f(x0)+(x–x0)f′(x0)=0，整理后得到 $x = {x_0} – f({x_0})/f’({x_0})$

但是因为我们用的是一阶泰勒展开，只能近似相等，求得的解并不能完全使f(x)=0成立。记这个金近似解为x1，只能说f(x1)比f(x0)更接近f(x)=0的解，因此我们可以写成一个递推公式：

${x_{k + 1}} = {x_k} – f({x_k})/f’({x_k}) \tag{3}$

通过迭代, 这个式子必然在f(x∗)=0f(x∗)=0的时候收敛。 整个过程如下图：

![2018tu353](https://github.com/appletrue/NoteML/blob/master/PICs/2018tu353.jpg)

**（2）二阶优化方法——牛顿法** 
对于非线性优化问题，牛顿法提供了一种求解的办法。假设任务是优化一个目标函数f(x), 求函数f(x)的极大或者极小问题, 可以转化为求解函数的导数f′(x)=0的问题, 这样求可以把优化问题看成方程求解问题。剩下的问题就和第一部分提到的牛顿迭代法求解很相似了。

我们先直接套用一下前面公式(3)，但是现在我们要把f(x)想象成f′(x)，要求f′(x)=0的根，则： 

${x_{n + 1}} = {x_n} – \frac{f'({x_n})}{f''({x_n})}$

也可以从泰勒展开获得推导：泰勒展开到二阶：

$f(x + \Delta x) = f\left( x \right) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta {x^2}$

$f(x + \Delta x) - f\left( x \right) = f'(x)\Delta x + \frac{1}{2}f''(x)\Delta {x^2}$

假设我们要求f(x)的极小值（注：我们并没有假设f(x)是convex的，所以可能是一个局部极小值。如果有一定基础的同学，可以把f(x)理解为loss function），并且希望一步就要走到尽可能使f(x)变小最多（loss尽可能低），那么记，

$F= f'(x)\Delta x + \frac{1}{2}f''(x)\Delta {x^2} \leq 0$

，并且绝对值最大，所以就是求F最小值。那就容易了，回到了初中数学~~~~对F求导等于0，得到 

$\Delta x{\rm{ = - }}\frac{{f'({x})}}{{f''({x})}}$

得出牛顿法迭代公式：${x_{n + 1}} = {x_n}{\rm{ - }}\frac{{f'({x_n})}}{{f''({x_n})}},n = 0,1,...$

如果只泰勒展开到一阶，那么上面的$f(x + \Delta x) - f\left( x \right)= f'(x)\Delta x \leq 0$，并且我们要求f′(x)Δx的最小值，Δx的模如果是一个常数，那么它的方向是−f(′x)的方向时， 取到最小值。所以梯度下降的公式就是：${x_{n + 1}} = {x_n} - \eta \cdot f'(x)$。其中η是步长，表示每次沿着负梯度方向走多远。）

对于高维函数，牛顿法通用公式写成： ${x_{n + 1}} = {x_n} - {[Hf({x_n})]^{ – 1}}\nabla f({x_n}),n \ge 0$

一般认为牛顿法可以利用到曲线本身的信息, 比梯度下降法更容易收敛（迭代更少次数）, 如下图是一个最小化一个目标方程的例子, 红色曲线是利用牛顿法迭代求解, 绿色曲线是利用梯度下降法求解：

![20180tu4](https://github.com/appletrue/NoteML/blob/master/PICs/20180tu4.jpg)

牛顿法在多变量问题上仍然适用迭代求解，但Hessian矩阵的引入增加了复杂性，特别是当： 
▪ Hessian 矩阵非正定，导致目标函数不一定下降，从而牛顿法不收敛； 
▪ Hessian 矩阵维度过大带来巨大的计算量。

针对这个问题，在牛顿法无法有效执行的情况下，提出了很多改进方法，比如 拟牛顿法（Quasi-Newton Methods）可以看作是牛顿法的近似。拟牛顿法只需要用到一阶导数，不需要计算Hessian矩阵 以及逆矩阵，因此能够更快收敛，关于拟牛顿法这里不再具体展开，也有更深入的 DFP、BFGS、L-BFGS等算法，大家可以自行搜索学习。总体来讲，拟牛顿法都是用来解决牛顿法本身的复杂计算、难以收敛、局部最小值等问题。


------------来源网址---------------------------------

[凸优化，Hessian，牛顿法](https://blog.csdn.net/xbinworld/article/details/79113218)

