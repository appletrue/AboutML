# 贝叶斯估计与极大似然估计

统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。

| 区别    | 极大似然估计                         | 贝叶斯估计                                    |
| ----- | ------------------------------ | ---------------------------------------- |
| 派别    | 频率派                            | 贝叶斯派                                     |
| 参数    | 认为参数是固定的                       | 认为参数也是服从某种概率分布的                          |
| To do | 根据已经掌握的数据来估计这个参数               | 已有的数据只是在这种参数的分布下产生的                      |
| 直观理解  | 极大似然估计就是假设一个参数 θ，然后根据数据来求出这个θ. | 难点在于p(θ) 需要人为设定，之后再考虑结合MAP （maximum a posterior）方法来求一个具体的θ |
| 不同    | 不考虑先验概率                        | 考虑先验概率（ 先验一定是与数据无关的，在 **没有任何数据之前**你就猜了一个参数的先验概率） |
| 适用范围  | 适用于 数据大量，估计的参数能够较好的反映实际情况      | 在数据量较少或者比较稀疏的情况下，考虑先验来提升准确率              |

### 问题前提：

有一堆数据D={x1,x2,...,xn}，假设这些数据是以含有未知参数θ 某种概率形式（如Bernoulli分布即0-1分布）分布的。**通过已有的数据，来估计这个未知参数θ**。

估计这个参数的好处就在于，我们可以对外来的数据进行预测。

### 例子：

假设一个抛硬币实验，我们之前不知道这些硬币是不是正反均匀的，也许硬币正反不等，假设正面向上设为1的概率为ρ，反面向上设为0为（1−ρ）. 我们进行了3次实验，得到两次正面，一次反面，即序列为′110′。这里，D=(1,1,0)，θ=ρ。

| 符号        | 含义                             |
| --------- | ------------------------------ |
| D         | 已有的数据(data)                    |
| θ         | 要估计的参数(parameter)              |
| p(θ)      | 先验概率(prior)                    |
| p(θ\|D)   | 后验概率(posterior)                |
| p(D)      | 数据分布(evidence)                 |
| p(D\|θ)   | 似然函数(likelihood of θ w.r.t. D) |
| p(x,θ\|D) | 已知数据条件下的x,θ概率                  |

极大似然估计和贝叶斯估计，要注意到两种方法在面对未知参数θ 时采用的不同态度。

### 极大似然模型推导：

极大似然估计法认为参数是固有的，但是可能由于一些外界噪声的干扰，使数据看起来不是完全由参数决定的。虽然有误差存在，**但只要让在这个数据给定的情况下，找到一个概率最大的参数**即可。问题其实就变成了一个条件概率最大的求解，即求使得p(θ|D) 最大的参数θ，即$\underset \theta{\arg\max} p(\theta \mid D)\tag{1}$。

根据条件概率公式有$p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}.\tag{2}$

在极大似然估计中假设 θ 是确定的，所以p(θ)就是一个常数。

p(D) 同样是根据已有的数据得到的，也是确定的，或者我们可以把其看作是对整个概率的一个归一化因子。转换为求解(3)式:

$\underset \theta {\arg max}   p(D|\theta)\tag{3}$

(3) 式中的p(D|θ) 就是似然函数，求一个使似然最大的参数，所以称为极大似然估计。 
想求解需要假设数据是相互独立的。$D={x_1,x_2,x_3,...,x_n}$，这时候有(4)式

$，p(D|\theta)=\prod_{i=1}^{n}p(x_i|\theta)\tag{4}，$

一般对(4)式取对数求解对数极大似然，就可以把连乘变成求和，然后求导取极值点就是要求的参数值。

**抛硬币实验**中，D=(1,1,0) , θ=ρ ，可得：

$\begin{align} p(D|\theta) &= p(x_1|\rho)p(x_2|\rho)p(x_3|\rho)\\ &=p(1|\rho)p(1|\rho)p(0|\rho)\\ &=\rho*\rho*(1-\rho) \end{align}\tag{5}$

使用对数极大似然估计就可以得到参数ρ 的值。

## 贝叶斯估计

先验概率(prior)与后验概率(posterior)简称为**先验**和**后验**。这两个概念来自贝叶斯定理。两者共同的对象，来自参数θ。

后验概率是指掌握了一定量的数据后，得到参数是如何分布的，表示为p(θ|D)。

先验就是在没有掌握数据后我们的参数怎么分布，很多时候完全是假设，然后去验证有的数据是否吻合先验猜想。**注意，先验一定是与数据无关的**，不能看到数据再做猜想，一定是**没有任何数据之前**你就猜了一个参数的先验概率。

### 贝叶斯估计模型的推导

公式(2) 其实是一个很概括的模型，既没有对概率形式以及概率参数进行定义，也没有运用到参数固定与否的思想，所以公式(2) 同样适用于贝叶斯模型。

$p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}.\tag{2}$

除了分母可以看作是一个归一化因子外，其余均是概率分布的函数。也就是说，无法再像极大似然估计那样将先验概率(p(θ))看作一个常量。

考虑用到先验概率，把分母也展开来看看，根据全概率公式得到，

$p(D)=\int_{\theta}p(D|\theta)p(\theta)d\theta\tag{6}.$

将（4）和（6）代入（2）可推导完成：

$p(\theta|D)=\frac{(\prod_{i=1}^{n}p(x_i|\theta))p(\theta)}{\int_{\theta}(\prod_{i=1}^{n}p(x_i|\theta))p(\theta)d\theta}\tag{7}$

 回到实例,上式中, 先验概率是已知的。

在解决此类贝叶斯估计时，参数以某种概率密度函数分布，会导致在计算过程中不可避免的高复杂度，为了计算上的方便，不再是把所有的后验概率p(θ|D)都找出来，仍然采用类似于极大似然估计思想，来得到**极大后验概率(Maximum A Posterior)**。

### **极大后验概率(MAP)**

MAP（Maximum A Posterior）的理论依据是绝大部分情况下，参数值最有可能出现在概率最大点附近。

抛硬币试验中，得到了数据D=(110)，如何预测第4次结果？当然需要计算p(1|D)和p(0|D)看看谁大谁小，哪个更有可能发生。

**已知数据D=(x1,x2,...,xn)，预测新的数据x的值。**

预测新的数据的值，其实就是能够在**已知数据D的情况下，找到数据的数学期望**,即求

$E(x|D)=\int_xxp(x|D)dx. \tag{9}$

(9)式中,需要求p(x|D)，式子中，内藏了一个参数，x 的分布与参数是有关的，而参数θ是服从某种概率分布的，考虑全参数所有可能，得到下式子：

$p(x|D)=\int_{\theta}p(x,\theta|D)d\theta \tag{10}$

运用基本的条件概率公式:

$p(x,\theta|D)=p(x|\theta,D)p(\theta|D). \tag{11}$

(11)式,即x和θ在已知数据D的条件下的概率，等于x在已知θ和数据D的条件下的概率乘θ在已知数据D的条件下的概率, 而$p(x|\theta,D)=p(x|\theta)$简化如此,(因为从数据里面得到的东西对一个新的数据来说，其实只是那些参数，所以对x而言，θ就是D，两者是同一条件。)

(10)变成(12), 

$p(x|D)=\int_{\theta}p(x,\theta|D)d\theta=\int_{\theta}p(x|\theta)p(\theta|D)d\theta.\tag{12}$

p(x|θ)是已知的(*例如在我们的问题里面可以是p(1|ρ)或者p(0|ρ)*)；p(θ|D)也是已知的，我们在贝叶斯估计中已经通过(7)式求出来了。所以这个式子完全就是一个只含有x的函数，带入(9)式完全可以计算出来数学期望。

### BUT

有一个细节 : 参数是随机分布的，我们需要考虑到每一个可能的参数情况然后积分，这种数学上的简单形式，其实想要计算出来需要大量的运算。

转换思路:找一个跟你差不多效果的后验概率，然后就只计算这个后验带入计算,即**找一个θ能够最大化后验概率**。

### **MAP算法**

最大化后验概率即最大化(7)式，分母只是一个归一化的因子，并不是θ的函数。真正有效的其实就是要最大化我们的分子，

$\theta_{MAP}=\arg\max_{\theta}\prod_{i=1}^{n}p(x_i|\theta)p(\theta)\tag{13}$

与极大似然估计形式上很相似，区别在于运用了先验概率在极大化里面。参数都已经计算出来了，其他过程按照极大似然来做就行，不用再按照贝叶斯一样对所有可能的参数情况都考虑在求积分了。

在进行参数估计的过程中，极大似然估计是让似然函数极大化，而考虑了MAP算法的贝叶斯估计，是让后验概率极大化。主要区别在于估计参数中，一个考虑了先验一个没有考虑先验，主要区别看(3)，(13)式。
