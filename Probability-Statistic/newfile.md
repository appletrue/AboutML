高斯分布分布，就是说刚才提到的分布都比较简单，我们能不能**把它们整合起来，设计我想要的分布**。这就用到高斯混合模型，这个图里面他构造了三种概率分布：

- 第一种的表述是“**各向同性**”，其中x1、x2两个变量的分布的方差，必须一样。那么从整个形成的几何形状看来，这些数据点就像一个球形或者是圆形。每一个方向的方差是一样的，是规则的形状。如果不满足就变成二和三的情形。
- 第二组是用一个**对角阵**，就是x1和x2在方阵的对角线上，其他位置是零，控制y这个维度上面的方差，把它放大了；相当于把第一种的变化做了一下拉伸。
- 第三种情况类似的，把X轴也做一下拉伸；当然在Y轴方向也有拉伸，这个是说x1、x2两个变量的方向可以做任意的控制，这就是高斯混合模型的作用，可以按照你想要的分布去设计。

标准正态分布。一倍标准差、两倍、三倍的位置对应的面积不同，分别覆盖了68%、95%、99.7%。三倍标准差以外的事件就当作小概率事件，这也是它的定义方式。





常用函数，这是一个sigmoid，它有饱和特性。

还有一个softplus，它是softmax的一种弱化；softmax从右往左下降会直接到0，在0的位置有一个突变，然后继续走；0这个点的左导数和右导数是不一样的，左导数是0，右导数是1，所以0这个点上的导数是不存在的。怎么办呢？为了数学上面好看，而且求导方便，那就把它变成softplus，在0这个点做变换之后就整个平滑起来，每个点的都是可导的。实际上在书里面也提到一点，平时其实深度网络DNN里面会经常用到ReLU，ReLU里就是softmax。softmax是ReLU的一种推广。ReLU里0点也是不可导的，就有一些规则的方法，就是如果到了这个点的话，他会给要么是0，要么是1，视具体情况而论。





这是贝叶斯规则，就是条件概率。x和y是两个随机变量，y发生的情况下x会发生的概率是 x单独发生的概率乘x发生的情况下y发生的概率，除以y单独发生概率。一般拿这个做一些判别分类。机器学习里面分两大类生成式和判别式，判别式的一个典型就是贝斯规则；生成式的方法跟判别式方法区别就是，生成式尽可能用模型去拟合它的联合分布，而判别式拟合的是一种条件分布。

贝叶斯学派和频率学派最大的不同、根上的不同，就是在于模型 y=wx+b 其中的w和b两个参数，频率学派认为参数是固定的，只要通过不停的采样、不停的观测训练，就能够估算参数w和b，因为它们是固定不变的；而贝叶斯学派相反，他们认为这些参数是变量，它们是服从一定的分布的，这是它最根本的差别。在这个基础上演变的最大似然估计、或者MAP等等的都不一样。这完全是两个不同的流派。

香农和三种书里面提到的三种特性：

- 非常可能发生的事件，它的信息量比较少，因为它确定性比较高；
- 而不可能发生的，或者是很少发生的，它的信息量就比较大；
- 独立事件具有增量的信息，刚才说的下雨就是一个例子；另一个例子是太阳从东边升起和从西边升起，这两个事件是完全独立的，两个事件的信息量可以累加起来。

这是信息论的几个概念，自信息、互信息、条件熵 。上面的公式是自信息的标准，直接就取一个对数而已，加上负号。熵就是把多种情况累加起来再取均值。

交叉熵，也是重点提到的概念。这是衡量事件发生的概率，像左侧靠近零，说明这个事件发生的可能性很小，那么它对应的信息量较少；然后到中间0.5的地方，比如说扔硬币有两种结果，两种结果0.5基本上靠猜，完全随机了；对于这样分不清到底结果是什么样的，对应的信息量最大的；类似的到另外一个极端，就是这个事件确定是可以发生的，可能性很大的，那信息量也小。

KL散度，基本上是衡量两个概率分布的差异。这个公式也很复杂，你们自己去琢磨，必须要看，看一遍然后才有直观的理解。现在讲也讲不清楚。（注：信息论也可以形象起来，参考：colah's blog,Visual Information Theory）

机器学习里面还有一个交叉熵，cross-entropy，跟熵是密切相关的，它的差别就是少了一项。

KL散度，它是不对称的，就是说概率p和概率q的顺序调一下是不同的概念，两个顺序不同要用于不同的场景。它的目标是要构造一个概率分布 q，去近似拟合、去模拟另外一个概率分布p。这个p分布是由两个正态分布组合起来的，两个叠加起来。怎么用q拟合它呢，如果用左边的散度去度量，算分布之间的误差，这个误差对应的就是KL散度，然后根据KL散度去有方向地去调整。这是它的过程，类似于机器学习里面的过程。

如果用左边的KL散度，p在前q在后，那我们会得到这样一个结果；绿色的是拟合的概率。它的效果是保证在高概率的地方，拟合的概率要高，而不考虑低概率的部分，所以结果就会做一个平滑。概率的总和还是1，要保证归一性嘛。右边反过来，q在前p在后，那么低概率要优先保证，高概率就忽略了，那么这个拟合的概率分布就尽量往一个峰靠，只能保证一个峰。这就解释了KL散度不对称性的应用，可以按照不同的应用场景取不同的方向。
