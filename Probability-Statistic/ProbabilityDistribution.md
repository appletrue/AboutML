# 统计推断常用概率分布总结

随机变量分布函数

 1）累积分布函数(Cumulative Distribution Function (CDF)) 

$F(x) = P(X ≤ x)$

2）概率密度函数(Probability Density Function (PDF)) 

$f(x) = \dfrac{dF(x)}{dx}$ 

$F(x) = \int_{-\infty}^{X}f(t)dt$

------

**离散型**随机变量分布:

- 0-1分布(伯努利分布)
- 二项分布(n重伯努利分布)
- 泊松分布

------

### 0-1分布（伯努利分布）

是**二项分布的特殊情况**，描述了二值随机变量的性质，它是离散型随机变量分布，是试验一次的二项分布

$p(X=k)=p^k(1−p)^{1−k}$ ,其中,k=0,1.

伯努利分布未必一定是 0-1 分布，也可能是 a-b 分布，只需满足**相互独立、只取两个值**的随机变量通常称为伯努利（Bernoulli）随机变量.

- $f(x) = \begin{cases}p &x= 1 \\1-p & x = 0 \\0 &otherwise\end{cases}$分段函数


- 均值：E(x)=p
- 方差：var(x)=p(1−p)

在机器学习领域中，是经典二分类算法-logisitic回归的概率基础

### 二项分布（n 重伯努利分布）

离散型随机变量分布，是N次伯努利实验得到的结果，其中结果只有两种，结果之间相互独立。（其实二项分布并不是一正一反的感觉，容易让人误解）

$p(X=k)=C_k^Np^k(1−p)^{n−k}$ ,记为$b(k;n,p)$

这里的描述就是，N次试验中取k次是成功的，所以得到的概率。为了严谨，所以还要乘上相反的概率。

- 期望：$μ=np$（即渴望得到p结果的均值）
- 方差：$σ^2=np(1−p)$

例:进行两次掷骰子（N=2），点数之和为2~12的概率.以掷骰子的点数和思考，当N趋近于无穷大的时候，二项分布所得到的骰子和非常的多，所以会被刻画出高斯分布的样子

##### 0-1 分布 & 二项分布

n 个彼此独立的均服从 0-1 分布的随机变量，$\{X_1,X_2…,X_n\}$，

- $∏_{i=1}^{N}p^{x1}(1−p)^{1−x_1}$：刻画的是联合分布（joint distribution）
- $\dbinom{n}{k}p^k(1−p)^{N−k}$：刻画的则是 n 重伯努利**相加**；

推广:

#### 多项式分布 Multinomial

多项式分布是二项分布的推广，仍然是进行n次独立实验，但是每次实验的结果不再只有两种，而是可以有m种。这m种结果彼此互斥，且发生的概率之和为1。 
多项式分布的概率密度函数是： 

$f(X)=f(x_1,x_2,...x_m) = C^{x1}_np^{x1}_1C^{x2}_{n−x1}p^{x2}_2...C^{x_m}_{n−x_1−x_2−..x_m−1}p_m^{x_m}$

$=\dfrac{n!}{x_1!x_2!...x_n!}p^{x_1}_1p^{x^2}_2...p^{x^m}_m$

$=\dfrac{n!}{x_1!x_2!...x_n!}∏_{i=1}^mp^{x_i}_i$     (多项式系数展开后可以约掉)

其中$∑_{i=1}^mx_i=n,∑_{i=1}^mp_i=1$

经典案例是掷骰子，骰子的每个面朝上的概率分别为$\{p_1,p_2,...,p_6\}$，特别的，这些概率值并不需要相等，只要每个面朝上这个事件彼此独立的就可以了，比如掷一个不规则的骰子。

### 泊松分布(Poisson分布)

泊松分布是二项分布的极限情况.**泊松分布就是描述某段时间内，事件具体的发生概率。**

$P(N(t)=n)=\dfrac{(\lambda t)^ne^{-\lambda t}}{n!}$

等号的左边，P 表示概率，N表示某种函数关系，t 表示时间，n 表示数量，如1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) 。等号的右边，λ 表示事件的频率。

在频率附近，事件的发生概率最高，然后向两边对称下降，即变得越大和越小都不太可能。每小时出生3个婴儿，这是最可能的结果，出生得越多或越少，就越不可能

------

连续型随机变量概率密度

- 均匀分布
- 指数分布
- 正态分布

------

均匀分布



### 指数分布

**指数分布是事件的时间间隔的概率。**如网站访问的时间间隔,婴儿出生的时间间隔等

指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间 t ，就等同于 t 之内没有任何婴儿出生

$P(X>t)=P(N(t)=0)=\dfrac{(\lambda t)^0 e^{-\lambda t}}{0!}=e^{-\lambda t}$

反过来，事件在时间 t 之内发生的概率，就是1减去上面的值。

$P(X≤t)=1-P(X>t)=1-e^{-\lambda t}$

指数分布的图形,体现随着间隔时间变长，事件的发生概率急剧下降，呈指数式衰减。想一想，如果每小时平均出生3个婴儿，上面已经算过了，下一个婴儿间隔2小时才出生的概率是0.25%，那么间隔3小时、间隔4小时的概率，是不是更接近于0？

### 正态分布

来源：中心极限定理 

- 定义：大量独立的随机变量之和趋向于正态分布（高斯分布）
- 前提：样本之间相互独立

 概率密度函数（PDF）

$f(x|\mu,\sigma^2)=\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\tfrac{(x-\mu)^2}{2\sigma^2}}$,结果可表示为 X~$N(\mu,\sigma^2)$

- 可以看出期望 μ 代表了正态分布的偏移量（位置）；方差$σ^2$代表了幅度
- 当μ=1，σ=0就是**标准正态分布（standard normal distribution）**,表示为N(0,1) ,$\dfrac{x-\mu}{\sigma}$~(0,1)

扩充：为什么测量误差服从正态分布 

- 误差公式：$ x¯−x=(\frac1{N}∑Ni=1xi)−x∗ (这里的x∗指的是真实值)
- 证明：由于每次测量误差都和其余测量误差的大小无关，因此是独立条件，所以 ∑Ni=1xi就是独立同分布的，乘以1N并不影响，减去x∗只改变了偏移量也不影响它的性质，因此测量误差服从独立同分布
- **在误差服从正态分布的情况下，测量量仍旧可以为其他分布**


