# 统计推断常用概率分布总结

随机变量分布函数

 1）累积分布函数(Cumulative Distribution Function (CDF)) 

$F(x) = P(X ≤ x)$

2）概率密度函数(Probability Density Function (PDF)) 

$f(x) = \dfrac{dF(x)}{dx}$ 

$F(x) = \int_{-\infty}^{X}f(t)dt$

------

**离散型**随机变量分布:

- 超几何分布
- 0-1分布(伯努利分布)
- 二项分布(n重伯努利分布)
- 泊松分布

------

### 超几何分布

超几何分布尤其指在抽样试验时抽出的样品不再放回去的分布情况。超几何分布是在知道样本所源自的总体中的项总数时，可对固定样本数量中的事件数量建模的离散分布。样本中每个项都具有两种可能的结果（事件或非事件）。样本没有被替换，因此，样本中的每个项都是不同的。

在从总体中选择项后，就不能再次选择该项了。因此，项被选中的几率会随着每一次试验不断增加（如果尚未选择此特定项）。

在不进行替换的情况下，超几何分布可用于从相对较小的总体中提取的样本。例如，超几何分布用在 Fisher 精确检验中以检验两个比例之间的差异，并按属性抽样验收，以从有限大小的孤立批次中进行抽样.

** 总结：**符合下面4个特点就是几何分布：

- 做某件事的次数是固定的，即试验次数是固定的;
- 每一次事件都有两个可能的结果;
- 每一次成功的概率都是想等的;
- **描述 进行x次尝试发生这个事件，取得首次成功的概率是多大。**

例子：抛硬币游戏，抛5次硬币，只有第5次正面朝上的概率多大？即前面4次，得到的结果都是反面朝上。

公式$P(x)=(1-p)^{x-1}p$ ,p为成功概率,即为了在第x 次尝试取得成功,首先要失败(x-1)次.

超几何分布的期望E(x)=1/P,标准差 $\sigma(x)=(1-p)/p^2$

##### 超几何分布和二项分布

超几何分布和二项分布都描述了在固定试验数中事件发生的次数。对于二项分布，每个试验的概率是相同的。对于超几何分布，每次试验都会改变每次后续试验的概率，因为不存在替换。

在超几何分布中，特别强调的是抽出的样品在下一次抽取前不再放回去，但是如果抽取的次数n和总共样品数N相比很小(大约n / N < 0,05)，这时在计算上二项分布和超几何分布相互间则没有主要的区别，此时人们更愿意采用二项分布的方法，因为在数学计算上二项分布要简单一些。

### 0-1分布（伯努利分布）

是**二项分布的特殊情况**，描述了二值随机变量的性质，它是离散型随机变量分布，是试验一次的二项分布。

$p(X=k)=p^k(1−p)^{1−k}$ ,其中,k=0,1.

伯努利分布未必一定是 0-1 分布，也可能是 a-b 分布，只需满足**相互独立、只取两个值**的随机变量通常称为伯努利（Bernoulli）随机变量.

- $f(x) = \begin{cases}p &x= 1 \\1-p & x = 0 \\0 &otherwise\end{cases}$分段函数

- 均值：E(x)=p
- 方差：var(x)=p(1−p)

在机器学习领域中，是经典二分类算法-logisitic回归的概率基础

### 二项分布（n 重伯努利分布）

离散型随机变量分布，是N次伯努利实验得到的结果，其中结果只有两种，结果之间相互独立。（其实二项分布并不是一正一反的感觉，容易让人误解）

$p(X=k)=C_k^Np^k(1−p)^{n−k}$ ,记为$b(k;n,p)$

这里的描述就是，N次试验中取k次是成功的，所以得到的概率。为了严谨，所以还要乘上相反的概率。

- 期望：$μ=np$（即渴望得到p结果的均值）
- 方差：$σ^2=np(1−p)$

例:进行两次掷骰子（N=2），点数之和为2~12的概率.以掷骰子的点数和思考，当N趋近于无穷大的时候，二项分布所得到的骰子和非常的多，所以会被刻画出高斯分布的样子

** 总结：**符合以下4个特点即是二项分布:

- 做某件事的次数是固定的;
- 每一次事件都有两个可能的结果;
- 每一次成功的概率都是相等的;
- **描述成功 x 次的概率是多少**?

例子：抛5次硬币，有2次反面朝上的概率是多少？

##### 0-1 分布 & 二项分布

n 个彼此独立的均服从 0-1 分布的随机变量，$\{X_1,X_2…,X_n\}$，

- $∏_{i=1}^{N}p^{x1}(1−p)^{1−x_1}$：刻画的是联合分布（joint distribution）
- $\dbinom{n}{k}p^k(1−p)^{N−k}$：刻画的则是 n 重伯努利**相加**；

### 泊松分布(Poisson分布)

泊松分布是__二项分布__的极限情况.

- **泊松分布就是描述某段时间内，事件具体的发生次数的概率。**
- **事件是独立事件**
- **同等时间段（周期）内，事件发生次数的概率分布相同**

例子:促销抽奖活动,活动一天平均中奖人数是5人,计算一天内有10人中奖的概率多大.

$P(N(t)=n)=\dfrac{(\lambda t)^ne^{-\lambda t}}{n!}$

等号的左边，P 表示概率，N表示某种函数关系，t 表示时间，n 表示数量，如1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) 。等号的右边，λ 表示事件的频率。

在频率附近，事件的发生概率最高，然后向两边对称下降，即变得越大和越小都不太可能。每小时出生3个婴儿，这是最可能的结果，出生得越多或越少，就越不可能

**泊松分布的期望**

$E(X)=\sum_{k=0}^{\infty }k\cdot \frac{\lambda ^{k}e^{-\lambda }}{k!}$

因为k=0时：$k\cdot \frac{\lambda ^{k}e^{-\lambda }}{k!}=0$

$E(X)=\sum_{k=1}^{\infty }k\cdot \frac{\lambda ^{k}e^{-\lambda }}{k!}$

变换：
$E(X)=\sum_{k=1}^{\infty }k\cdot \frac{\lambda ^{k}e^{-\lambda }}{k!}=\sum_{k=1}^{\infty } \frac{\lambda ^{k}e^{-\lambda }}{(k-1)!}=\sum_{k=1}^{\infty } \frac{\lambda ^{k-1}\lambda e^{-\lambda }}{(k-1)!}=\lambda e^{-\lambda }\sum_{k=1}^{\infty } \frac{\lambda ^{k-1}}{(k-1)!}$

需要用到泰勒展开式，常用的泰勒展开式中：

$e^{x}=1+x+\frac{x^{2}}{2!}+\frac{x^{3}}{3!}+...+\frac{x^{n}}{n!}+...=\sum_{k=1}^{\infty } \frac{x ^{k-1}}{(k-1)!}$

因此，泊松分布的期望为: $E(X)=\lambda e^{-\lambda }\sum_{k=1}^{\infty } \frac{\lambda ^{k-1}}{(k-1)!}=\lambda e^{-\lambda }e^{\lambda }=\lambda$

**方差D(X)**，先求出$E(X^2)$: 

$E(X^2)=\sum_{k=0}^{\infty }k^2 \cdot \frac{\lambda ^{k}e^{-\lambda }}{k!}=\lambda e^{-\lambda} \sum_{k=1}^{\infty } \frac{k \lambda ^{k-1}}{(k-1)!}=\lambda e^{-\lambda} \sum_{k=1}^{\infty } \frac{(k-1+1) \lambda ^{k-1}}{(k-1)!}$

$=\lambda e^{-\lambda} (\sum_{m=0}^{\infty } \frac{m \cdot \lambda ^{m}}{m!}+\sum_{m=0}^{\infty } \frac{ \lambda ^{m}}{m!}) (m=k-1)$

$=\lambda e^{-\lambda} ( \lambda \cdot \sum_{m=1}^{\infty } \frac{\lambda ^{m-1}}{(m-1)!}+\sum_{m=0}^{\infty } \frac{ \lambda ^{m}}{m!})$

$=\lambda e^{-\lambda}(\lambda e^{\lambda}+e^\lambda)=\lambda(\lambda+1)$

所以:$D(X)=E(X^2)-(E(X))^2=\lambda(\lambda+1)-\lambda^2=\lambda$

**泊松分布的期望和方差为: E(X)=λ,   D(X)=λ**

#### 推广:

#### 多项式分布 Multinomial

多项式分布是二项分布的推广，仍然是进行n次独立实验，但是每次实验的结果不再只有两种，而是可以有m种。这m种结果彼此互斥，且发生的概率之和为1。 
多项式分布的概率密度函数是： 

$f(X)=f(x_1,x_2,...x_m) = C^{x1}_np^{x1}_1C^{x2}_{n−x1}p^{x2}_2...C^{x_m}_{n−x_1−x_2−..x_m−1}p_m^{x_m}$

$=\dfrac{n!}{x_1!x_2!...x_n!}p^{x_1}_1p^{x^2}_2...p^{x^m}_m$

$=\dfrac{n!}{x_1!x_2!...x_n!}∏_{i=1}^mp^{x_i}_i$     (多项式系数展开后可以约掉)

其中$∑_{i=1}^mx_i=n,∑_{i=1}^mp_i=1$    

经典案例是掷骰子，骰子的每个面朝上的概率分别为$\{p_1,p_2,...,p_6\}$,

特别的，这些概率值并不需要相等，只要每个面朝上这个事件彼此独立的就可以了，比如掷一个不规则的骰子。

------

连续型随机变量概率密度
- 指数分布
- 均匀分布
- 正态分布
- 贝塔分布

------

### 指数分布

**指数分布是事件的时间间隔的概率**。如网站访问的时间间隔,婴儿出生的时间间隔等

指数分布有如下的适用条件： 

1. x是两个事件发生之间的时间间隔，并且x>0; 
2. 事件之间是相互独立的； 
3. 事件发生的频率是稳定的； 
4. 两个事件不能发生在同一瞬间。

这几个条件实质上也是使用泊松分布的前提条件。如果满足上述条件，则x是一个指数随机变量，x的分布是一个指数分布。如果不满足上述条件，那么需要使用Weibull分布或者gamma分布。

指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间 t ，就等同于 t 之内没有任何婴儿出生

$P(X>t)=P(N(t)=0)=\dfrac{(\lambda t)^0 e^{-\lambda t}}{0!}=e^{-\lambda t}$（如果x<=0，则f(x)=0，e则是自然对数的底数）

反过来，事件在时间 t 之内发生的概率，就是1减去上面的值。

$P(X≤t)=1-P(X>t)=1-e^{-\lambda t}$ 就变成了**时间间隔t在参数λ下的分布函数**

根据概率论知识，分布函数是概率密度函数从负无穷到正无穷上的积分。对上述的分布函数进行求导，得$f(t)=\lambda e^{-\lambda t}$ ,即从泊松分布推导出指数分布的概率密度函数.

指数分布只有一个参数，“λ”，λ是事件发生的频率，在不同的应用场景中可能有不同名称，如事件频率，到达频率，死亡率，故障率，转变率等。

#### 期望和方差推导:

首先，指数分布属于连续型随机分布，因此，其期望E(X)为

$E(X)=\int_{-\infty }^{\infty }|x|f(x)dx=\int_{0}^{\infty }xf(x)dx=\int_{0}^{\infty }x\cdot\lambda e^{-\lambda x}dx=\frac {1} {\lambda}\int_{0}^{\infty }\lambda xe^{-\lambda x}d\lambda x$

令u=λx，则： 

$E(X)=\frac {1} {\lambda}\int_{0}^{\infty }ue^{-u}du=\frac {1} {\lambda}[(-e^{-u}-ue^{-u})|(\infty,0)]=\frac {1} {\lambda}$

对于**指数分布的方差D(X)**：$D(X)=E(X^2)-(E(X))^2$

$E(X^2)=\int_{-\infty }^{\infty }|x^2|f(x)dx=\int_{0}^{\infty }x^2f(x)dx=\int_{0}^{\infty }x^2\cdot\lambda e^{-\lambda x}dx$

$E(X^2)=\frac {1} {\lambda^2}\int_{0}^{\infty }\lambda x \lambda xe^{-\lambda x}d\lambda x$

令u=λx，则：

$E(X^2)=\frac {1} {\lambda^2}\int_{0}^{\infty }u^2e^{-u}du=\frac {1} {\lambda^2}[(-2e^{-u}-2ue^{-u}-u^2e^{-u})|(\infty,0)]=\frac {1} {\lambda^2}\cdot 2=\frac {2} {\lambda^2}$

所以:$D(X)=E(X^2)-(E(X))^2=\frac {2} {\lambda^2}-(\frac {1} {\lambda})^2=\frac {1} {\lambda^2}$

指数分布的图形,体现随着间隔时间变长，事件的发生概率急剧下降，呈指数式衰减。想一想，如果每小时平均出生3个婴儿，上面已经算过了，下一个婴儿间隔2小时才出生的概率是0.25%，那么间隔3小时、间隔4小时的概率，是不是更接近于0？

### 均匀分布

设连续随机变量X的一切可能值充满一个有限区间[a,b],且在该区间内任意 点概率的密度相同,即密度函数f(x)在区间[a,b]上为常量,称此分布为均匀分布 ,记作U(a,b)

当X在[a,b]上服从分布U(a,b)时,记为X~U(a,b).

在区间[a,b]上服从均匀分布的随机变量X,落在区间[a,b]中任意等长度的子区间内的可能性是相同的.

1.概率密度

在区间[a,b]上概率密度f(x)=C(常数),于是 
定义积分操作为inte( a , b )[]  a,b为上下限

$inte(a,b)[C]dx = C*(b-a) = 1 => C = 1/(b-a)$

又因为随机变量X不可能取得区间[a,b]外的值,所以在[a,b]外,概率密度为0,于是概率密度为

$f(x) = { 1/(b-1) , a <= x <= b ;  0 , else }$

2.分布函数(就是到-oox的密度函数的积分

### 正态分布

来源：中心极限定理 

- 定义：大量独立的随机变量之和趋向于正态分布（高斯分布）
- 前提：样本之间相互独立

 概率密度函数（PDF）

$f(x|\mu,\sigma^2)=\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\tfrac{(x-\mu)^2}{2\sigma^2}}$,结果可表示为 X~$N(\mu,\sigma^2)$

![normal-dis](https://github.com/appletrue/NoteML/blob/master/PICs/normal-dis.jpg)

- 可以看出期望 μ 代表了正态分布的偏移量（位置）；方差$σ^2$代表了幅度
- 当μ=1，σ=0就是**标准正态分布（standard normal distribution）**,表示为N(0,1) ,$\dfrac{x-\mu}{\sigma}$~(0,1)

扩充：为什么测量误差服从正态分布 

- 误差公式：$ x¯−x=(\frac1{N}∑^N_{i=1}x_i)−x∗$ (这里的x∗指的是真实值)
- 证明：由于每次测量误差都和其余测量误差的大小无关，因此是独立条件，所以 ∑Ni=1xi就是独立同分布的，乘以1N并不影响，减去x∗只改变了偏移量也不影响它的性质，因此测量误差服从独立同分布
- **在误差服从正态分布的情况下，测量量仍旧可以是其他分布**

### gamma分布

### beta分布

beta分布可以看作一个概率的概率分布，当你不知道一个东西的具体概率是多少时，它可以给出了所有概率出现的可能性大小。


------参考网址-------     
[泊松分布的期望和方差及其推导](http://blog.csdn.net/saltriver/article/details/52969014)

[二项分布均值方差及其推导](http://blog.csdn.net/saltriver/article/details/52600094)


