## 岭回归 RidgeRegression

多元线性回归模型中，如果所有特征一起上，容易造成过拟合使测试数据误差方差过大；因此减少不必要的特征，简化模型是减小方差的一个重要步骤。除了直接对特征筛选，来也可以进行特征压缩，减少某些不重要的特征系数，系数压缩趋近于0就可以认为舍弃该特征。

岭回归（Ridge Regression）和Lasso回归是在普通最小二乘线性回归的基础上加上正则项以对参数进行压缩惩罚。

首先，对于普通的最小二乘线性回归，它的代价函数是：

$F =\sum_{i=1}^{n}(y_i - \beta _0 - \sum_{j=1}^p \beta_j x_{ij})$

通过拟合系数β来使F最小。方法很简单，求偏导利用线性代数解方程组即可。

根据线性代数的理论可知，只要样本量合适，它就存在唯一解，也就是该模型的最优解。但尽管使F达到了最小，它还是把所有的特征看作同样重要的程度来求解，并没有做任何特征选择，因此存在过拟合的可能。

**岭回归在OLS回归模型的F上加上了惩罚项（l2范数）**，这样代价函数就成为：

$\sum_{i=1}^{n}(y_i - \beta _0 - \sum_{j=1}^p \beta_j x_{ij})+ \lambda\sum_{j=1}^p\beta^2_j$ ------(岭回归的代价函数)

λ是一个非负的调节参数，可以看到：

当λ=0时，此时它与F一致，没有起到任何惩罚作用；

当λ -> ∞时，它的惩罚项也就是无穷大，而为了使代价函数最小，只能压缩系数β趋近于0。

但是因为λ不可能为无穷大，二次项求偏导时总会保留变量本身，所以事实上它也不可能真正地将某个特征压缩为0。尽管系数较小可以有效减小方差，但依然留着一大长串特征会使模型不便于解释。这是岭回归的缺点。

**lasso回归的正项则就把二次项改成了一次绝对值（l1范数）**，具体为：

$\sum_{i=1}^{n}(y_i - \beta _0 - \sum_{j=1}^p \beta_j x_{ij})+ \lambda\sum_{j=1}^p\mid\beta_j\mid$ 

一次项求导可以抹去变量本身，因此lasso回归的系数可以为0。这样可以起来真正的特征筛选效果。

无论对于岭回归还是lasso回归，本质都是通过调节λ来实现模型误差vs方差的平衡调整。

### 岭回归示例：

病态系统：现有现行系统，Ax=b，解方程 

$\begin{pmatrix}400&  -201\\  -800   & 401\end{pmatrix}\begin{pmatrix}x_1\\ x_2   \end{pmatrix}=\begin{pmatrix}200\\ -200  \end{pmatrix}$

很容易得到解为，x1 =-100,x2=-200,如果在样本采集时存在一个微小的误差，如，A矩阵的系数400变成401，

$\begin{pmatrix}401&  -201\\  -800   & 401\end{pmatrix}\begin{pmatrix}x_1\\ x_2   \end{pmatrix}=\begin{pmatrix}200\\ -200  \end{pmatrix}$

则得到一个截然不同的解，x1=40000,x2=79800

当解集x 对A 和b 的系数高度敏感，则这个方程组就是病态的。

$\begin{pmatrix}400+10&  -201\\  -800   & 400+10\end{pmatrix}$ 解为 x1=5.726,x2=10.685

$\begin{pmatrix}400+10&  -201\\  -801   & 400+10\end{pmatrix}$ 解为 x1=5.888,x2=11.016

因此，对岭回归的解法写作 $(x^TX + \lambda I) ^{-1}X^Ty$,即 把$X^TX$近半正定矩阵，用正定矩阵替代。
