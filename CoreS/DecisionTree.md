决策树

https://blog.csdn.net/ljp812184246/article/details/47402639

https://blog.csdn.net/Gamer_gyt/article/details/51242815

https://blog.csdn.net/bcqtt/article/details/52624407

https://blog.csdn.net/fly_time2012/article/details/70210725

https://blog.csdn.net/google19890102/article/details/28611225

https://www.jianshu.com/p/b7d71478370d

https://zhuanlan.zhihu.com/p/27905967

https://blog.csdn.net/xbinworld/article/details/44660339

https://blog.csdn.net/suipingsp/article/details/41927247

### 什么是决策树？

决策树来自决策论，由多个决策分支和可能的结果（包括资源成本和风险）组成，用来创建到达目标的规划。也可以用来表示算法

决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

决策树学习的目的：为了产生一颗泛化能力强的决策树，即处理未见示例能力强。

根节点：包含全部样本

叶节点：对应决策结果

内部节点：对应属性测试

####  实例

| ID   | 拥有房产（是/否） | 婚否（单身/已婚/离异） | 年收入（千） | 偿还债务（是/否） |
| ---- | --------- | ------------ | ------ | --------- |
| 1    | 是         | 单身           | 125    | 否         |
| 2    | 否         | 已婚           | 100    | 否         |
| 3    | 否         | 单身           | 70     | 否         |
| 4    | 是         | 已婚           | 120    | 否         |
| 5    | 否         | 离异           | 95     | 是         |
| 6    | 否         | 已婚           | 60     | 否         |
| 7    | 是         | 离异           | 220    | 否         |
| 8    | 否         | 单身           | 85     | 是         |
| 9    | 否         | 已婚           | 75     | 否         |
| 10   | 否         | 单身           | 90     | 是         |

决策树的训练数据往往就是这样的表格形式，表中的前三列（ID不算）是数据样本的属性，最后一列是决策树需要做的分类结果。通过该数据，构建的决策树如下： 

![20180423dt1](https://github.com/appletrue/NoteML/blob/master/PICs/20180423dt1.jpg)

有了这棵树，我们就可以对新来的用户数据进行是否可以偿还的预测了。

**决策树最重要的是决策树的构造。**

所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。

**构造决策树的关键步骤是分裂属性。**

所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况： 
1、属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。 
2、属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。 
3、属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。

决策树的属性分裂选择是”贪心“算法，也就是没有回溯的。

### ID3 算法：

信息论中有熵（entropy）的概念，表示状态的混乱程度，熵越大越混乱。

熵的变化可以看做是信息增益，决策树ID3算法的核心思想是以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。

设D为用（输出）类别对训练元组进行的划分，假定当前样本集合D中第i 类样本所占的比例为pi则D的熵表示为：$info(D)=-\sum^{m}_{i=1}p_i log_2 p_i$

一般用这个类别的样本数量占总量的占比来作为概率的估计；

熵的实际意义表示是D中元组的类标号所需要的平均信息量。

如果将训练元组D按属性A进行划分，则A对D划分的期望信息为： 

$info_A(D)=\Sigma _{j=1}^v  \dfrac{D_j}{D}info(D_j)$

于是，信息增益即两者的差值：$Gain(A)=info(D)-info_A(D)$

ID3.0决策算法就用到上面信息增益，在每次分裂时，谈心选择信息增益最大的属性，作为本次分裂属性。每次分裂就会使得树长高一层。这样逐步生产下去，就一定可以构建一颗决策树。

例子：输入样本的属性有三个——日志密度（L），好友密度（F），以及是否使用真实头像（H）；样本的标记是账号是否真实yes or no。以下信息增益的计算过程：

| 日志密度 | 好友密度 | 是否使用真实头像 | 账号是否真实 |
| ---- | ---- | -------- | ------ |
| s    | s    | no       | no     |
| s    | l    | yes      | yes    |
| l    | m    | yes      | yes    |
| m    | m    | yes      | yes    |
| l    | m    | yes      | yes    |
| m    | l    | no       | yes    |
| m    | s    | no       | no     |
| l    | m    | no       | yes    |
| m    | s    | no       | yes    |
| s    | s    | yes      | no     |

可以一次计算每一个属性的信息增益，比如日致密度的信息增益是0.276。

$info_L(D)= 0.3*(-\frac0 3 log_2 \frac0 3  -\frac3 3 log_2 \frac33 ) +0.4*(-\frac14 log_2 \frac14  -\frac3 4 log_2 \frac34 )+0.3*(-\frac1 3 log_2 \frac1 3  -\frac2 3 log_2 \frac23 )=0+0.326+0.277=0.603$

$Gain(L)=0.879-0.603=0.276$

同理 H和F的信息增益分别为 0.033 和0.553.因为F具有最大信息增益，所以第一次分裂选择F为分裂属性，分裂后结果如：

![20180423dt2](https://github.com/appletrue/NoteML/blob/master/PICs/20180423dt2.jpg)

上面为了简便，将特征属性离散化了，其实日志密度和好友密度都是连续的属性。对于特征属性为连续值，可以如此使用ID3算法：先将D中元素按照特征属性排序，则每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜在分裂点开始，分裂D并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。

**ID3.0 缺陷：**

选择的时候容易选择一些比较容易分纯净的属性，尤其在具有像ID值这样的属性，因为每个ID都对应一个类别，所以分的很纯净，ID3比较倾向找到这样的属性做分裂。

### C4.5

C4.5算法定义了分裂信息，表示为： $\begin{equation}split\_info_A(D) = -\sum_{j=1}^{v}\frac{|D_j|}{|D|}\log_2(\frac{|D_j|}{|D|})\end{equation}$

这个也是一个熵的定义，$p_i = \frac{|D_j|}{|D|}$pi=|Dj||D|，可以看做是属性分裂的熵，分的越多就越混乱，熵越大。定义信息增益率： $\begin{equation}gain\_ratio(A) = \dfrac{gain(A)}{split\_info(A)}\end{equation}$,C4.5就是选择最大增益率的属性来分裂，其他类似ID3.5。.







分类预测：决策树表示？



决策树的生成→对训练样本进行分组

- 关键：确定树根结点和分支准则
- 停止生长时机

决策树的修剪→解决过度拟合问题

- 预先修剪，限值决策树的充分生长，如：限制树的高度
- 滞后修剪：待决策树充分生长完毕后再进行修剪
  - 当结点和分支数较多时，显然不合适。





补充：信息熵

香农提出了“信息熵”概念，解决了对信息的量化质量问题，香农用“信息熵”来描述信源的不确定性。

“信息熵”（Information Entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第K类样本所占的比例为pi(k=1,2,..,|γ|)，则D的信息熵定义为

$Ent(D)=-\sum^{|γ|}_{i=1}p_i log_2 p_i$

Ent(D)的值越小，D的纯度越高。对于二分类任务，|γ|=2

假设已知衡量不确定性大小的这个量已经存在，则这个“信息量”：

- 不会是负数
- 不确定性函数f 是概率P的单调递减函数
- 可加性：两个独立符号所产生的不确定性等于各自不确定性之和，即 $f(p_1Xp_2)=f(p_1)+f(p_2)$

同时满足这3个条件的函数 f 是负的对数函数，即 $f(p_i)=log \frac{1}{p_i} =-log p_i$

一个时间的信息量就是该事件发生的概率的负对数。

信息熵是跟所有事件的可能性有关的，是平均而言发生一个事件得到的信息量大小。所以信息熵其实是信息量的期望。

$E[-log p_i] =-\sum^n_{i=1}p_i logp_i$

若一事件有k 种结果，对应的概率为$P_i$,则此事件发生后所得信息量I （视为Entropy）为：

$（I =-（p_1log_2(p_1)+p_2log_2(p_2)+...+p_klog_2(p_k))$





**ID3.0；C4.5；CART**

**ID3**算法主要针对属性选择问题，是决策树学习方法中最具影响和最为典型的算法。该方法使用信息增益度选择测试属性。

当获取信息时，将不确定的内容转化为确定的内容，因此信息伴着不确定性。

从直觉上讲，小概率事件比一般概率事件包含的信息量大。如果某件事情是“百年一见”则肯定比“习以为常”的事件包含的信息量大。

如何度量信息量的大小？

- 用**信息增益**度量**熵**的**降低程度**

以信息熵的下降速度作为选取测试属性的标准，所选的测试属性是从根节点到当前节点的路径上尚未被考虑的具有最高信息增益的属性。计算过程相关公式：

1，x是一个离散型的随机变量，其概率分布为 $p(x)=P(X=x),x∈X$,则X 的熵为 $H(X)=-\sum_{i=0}^np(x_i)log_2p(x_i)$,约定0log20=0。 

n 表示有多少个分类，可以理解为所选测试属性下有多少个不同值，H越大，信息的不确定性越大，H越小，信息的不确定性越小，H等于0时最具确定性，这种情况是所有元素都归为同一类，每个值的发生都有相同的概率。

2，参考至信息论中的信道的数学模型 ，有 
$U=[u1,u2,u3,...,ur]$，信源所发消息，离散随机变量； 
$V=[v1,v2,v3,...,vq]$，新宿所收消息，离散随机变量； 
$P(V|U)$：转移概率表示发生为U,接收为V的概率，(也可以这样理解，当一个属性中取值为U时，对另一个属性取值为V的影响程度），存在 

$∑(v_j|u_i)=1,i=1,2,...,r;j=1,2,...,q$

3，先验概率P(U)：信源发送消息前，U的概率分布。其实就是上面的p(x)。

4，后验概率P(U|vj) : 信宿端收到消息vj后，U的概率分布。

5，后验熵：信宿端收到vjvj后，关于U的平均不确定性为：

$H(U|v_j)=-\sum_{i=0}^rP(u_i|v_j)log_2P(u_i|v_j),j=1,2,...,q$

6，条件熵： 即信道疑异度，对后验熵在输出V集合中求期望值，表示在信宿端收到全部输出v后,对信源端U尚存在的不确定性。(H(U|V)<H(U))

$H(U|V)=-\sum_{j=0}^q\sum_{i=0}^rP(u_i|v_j)log_2P(u_i|v_j)$

7，信息增益：衡量通过信道传输进行通信后所消除的不确定性的大小，定义为：$I(U,V)=H(U)-H(U|V)$,表示收到V后获得关于U的信息量，是不确定性的消除是信宿端所获得的信息量，其中：

$H(U)=-\sum_{i=0}^rP(u_i)log_2P(u_i)$

$H(U|V)=-\sum_{j=0}^q\sum_{i=0}^rP(u_i|v_j)log_2P(u_i|v_j)$



属性A的信息增益，使用属性A分割样例集合S 而导致的熵的降低程度

$Gain(S,A)=Entropy(S)-\Sigma_{v \in Values(A)} \frac{|S_v|}{|S|}Entropy(S_v)$



**CART**

基尼值

$（Gini（D)=\Sigma_{k=1}^{|y|}\Sigma_{k'≠k}p_kp_k'=1-\Sigma_{k=1}^{|y|}p_k^2$

直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此，Gini(D)越小，则数据集D的纯度越高。

基尼指数

$Gini.index(D,a)=\Sigma_{v=1}^{V}\frac{D^v}{|D|}Gini(D^v)$

于是在候选属性集合A中，选择使得划分后基尼指数最小的属性作为最优划分属性，即

$a_k=\underset{a \in A}{ arg min} Gini.index(D,a)$



**过拟合和欠拟合**

过拟合：学习器学习能力过于强大， 把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质， 导致泛化性能下降。 

欠拟合： 学习器学习能力低下， 对训练样本的一般性质尚未学好。 

剪枝， 即通过主动去掉一些分支来降低过拟合的风险。

决策树的剪枝策略



预剪枝：在决策树生成过程中， 对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能升， 则停止划分并将当前结点标记为叶结点。

后剪枝：先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察， 若将该结点对应的子树替换为叶结点能带来决策树泛化性能ᨀ升， 则将该子树替换为叶结点。 

训练集**S**和测试集**T** ：$且D=S∪ T 且 S ∩ T ≠ Φ$



= [关于熵Entropy](https://github.com/appletrue/NoteML/blob/master/math/Entropy.md)
