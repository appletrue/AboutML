决策树

https://blog.csdn.net/ljp812184246/article/details/47402639

https://blog.csdn.net/Gamer_gyt/article/details/51242815

https://blog.csdn.net/bcqtt/article/details/52624407

https://blog.csdn.net/fly_time2012/article/details/70210725

https://blog.csdn.net/google19890102/article/details/28611225

https://www.jianshu.com/p/b7d71478370d

https://zhuanlan.zhihu.com/p/27905967

https://blog.csdn.net/xbinworld/article/details/44660339

https://blog.csdn.net/suipingsp/article/details/41927247

什么是决策树？

决策树来自决策论，由多个决策分支和可能的结果（包括资源成本和风险）组成，用来创建到达目标的规划。也可以用来表示算法

决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果

决策树学习的目的：为了产生一颗泛化能力强的决策树，即处理未见示例能力强。

根节点：包含全部样本

叶节点：对应决策结果

内部节点：对应属性测试



分类预测：决策树表示？



决策树的生成→对训练样本进行分组

- 关键：确定树根结点和分支准则
- 停止生长时机

决策树的修剪→解决过度拟合问题

- 预先修剪，限值决策树的充分生长，如：限制树的高度
- 滞后修剪：待决策树充分生长完毕后再进行修剪
  - 当结点和分支数较多时，显然不合适。







信息熵

香农提出了“信息熵”概念，解决了对信息的量化质量问题，香农用“信息熵”来描述信源的不确定性。

“信息熵”（Information Entropy）是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第K类样本所占的比例为pk(k=1,2,..,|γ|)，则D的信息熵定义为

$Ent(D)=-\sum^{|γ|}_{k=1}p_k log_2 p_k$

Ent(D)的值越小，D的纯度越高。对于二分类任务，|γ|=2

假设已知衡量不确定性大小的这个量已经存在，则这个“信息量”：

- 不会是负数
- 不确定性函数f 是概率P的单调递减函数
- 可加性：两个独立符号所产生的不确定性等于各自不确定性之和，即 $f(p_1Xp_2)=f(p_1)+f(p_2)

同时满足这3个条件的函数 f 是负的对数函数，即 $f(p_i)=log \frac{1}{p_i} =-log p_i$

一个时间的信息量就是该事件发生的概率的负对数。

信息熵是跟所有事件的可能性有关的，是平均而言发生一个事件得到的信息量大小。所以信息熵其实是信息量的期望。

$E[-log p_i] =-\sum^n_{i=1}p_i logp_i$

若一事件有k 种结果，对应的概率为$P_i$,则此事件发生后所得信息量I （视为Entropy）为：

$（I =-（p_1log_2(p_1)+p_2log_2(p_2)+...+p_klog_2(p_k))$





**ID3.0；C4.5；CART**

**ID3**算法主要针对属性选择问题，是决策树学习方法中最具影响和最为典型的算法。该方法使用信息增益度选择测试属性。

当获取信息时，将不确定的内容转化为确定的内容，因此信息伴着不确定性。

从直觉上讲，小概率事件比一般概率事件包含的信息量大。如果某件事情是“百年一见”则肯定比“习以为常”的事件包含的信息量大。

如何度量信息量的大小？

- 用**信息增益**度量**熵**的**降低程度**

属性A的信息增益，使用属性A分割样例集合S 而导致的熵的降低程度

$Gain(S,A)=Entropy(S)-\Sigma_{v \in Values(A)} \frac{|S_v|}{|S|}Entropy(S_v)$



CART

基尼值

$（Gini（D)=\Sigma_{k=1}^{|y|}\Sigma_{k'≠k}p_kp_k'=1-\Sigma_{k=1}^{|y|}p_k^2$

直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此，Gini(D)越小，则数据集D的纯度越高。

基尼指数

$Gini.index(D,a)=\Sigma_{v=1}^{V}\frac{D^v}{|D|}Gini(D^v)$

于是在候选属性集合A中，选择使得划分后基尼指数最小的属性作为最优划分属性，即

$a_k=\underset{a \in A}{ arg min} Gini.index(D,a)$



过拟合和欠拟合

过拟合：学习器学习能力过于强大， 把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质， 导致泛化性能下降。 

欠拟合： 学习器学习能力低下， 对训练样本的一般性质尚未学好。 

剪枝， 即通过主动去掉一些分支来降低过拟合的风险。

决策树的剪枝策略

预剪枝：在决策树生成过程中， 对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能ᨀ升， 则停止划分并将当前结点标记为叶结点。

后剪枝：先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察， 若将该结点对应的子树替换为叶结点能带来决策树泛化性能ᨀ升， 则将该子树替换为叶结点。 

训练集**S**和测试集**T** ：$且D=S∪ T 且 S ∩ T ≠ Φ$



= [关于熵Entropy](https://github.com/appletrue/NoteML/blob/master/math/Entropy.md)
