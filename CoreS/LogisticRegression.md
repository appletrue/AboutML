## Logistic回归

### 起源

logistic regression的起源主要分为几个阶段，从开始想到logistic这个词，到发现logistic function，再推导出logit function，最后才命名logistic regression。这些过程都是大量的研究者们共同努力发现的，只是在历史的长河中，很多人被渐渐遗忘了。

logistic起源于对人口数量增长情况的研究，最重要的工作是Pierre François Verhulst在1838年提出了对人口增长的公式描述。，他博士毕业于根特大学的数学系，是个数学教授和人口学家。在1835年Verhulst的同乡人Adolphe Quetelet发表了一篇关于讨论人口增长的文章，文中认为人口不可能一直是几何(指数)增长，而会被与增长速度平方成比例的一种阻力而影响，但是这篇论文只有猜想没有数学理论基础，却极大的启发了Verhulst。因此在1838年Verhulst发表了关于人口数量增长的论文，就是在这篇论文里面他推导出了logistic equation，文章中谈到一个重要观点，随着时间的增加，一个国家的大小（我理解为资源）和这个国家人们的生育能力限制了人口的增长，人口数量会渐渐趋近一个稳定值。厉害的是他将这个过程用公式给描述出来了，他从人口数量增长的速度公式入手，即人口数量P(t)P(t)对时间t的导数:

$\dfrac{\partial P}{\partial t} = rP(1-\dfrac{P}{K})$,K就是他认为人口数量稳定的值

当P(t)远小于K时，求导公式后一项约等于0，那么就变成了$∂P∂t≃rP$，这个阶段人口增长速度与人口数量和一个常数的乘积成正比，并且在渐渐变大。然后对这个式子求解一阶线性微分方程得到$P(t)≃P(0)e^rt$。当P(t)接近K时，人口增长速度开始渐渐变小，同样求解二阶微分方程，然后将二者整合在一起得到最初的形式。

$P(t) = \dfrac{P(0)e^{rt}}{1+P(0)(e^{rt}-1)/K}$

当时并没有那么多年的数据（但后人总结300年来的人口增长分布，非常漂亮的拟合了logisitc分布的累积分布函数走势），这个公式也没有命名，直到1845年他发表了另外一篇重要文章（文章中他发现在$P(t)K/2$时，P(t)呈凹增长(通过求二阶导来分析))，他命名公式——“logistic”，这个增长的趋势类似logistic分布的概率密度函数。

然而在后来的几十年内人们都没有意识到这个工作的重要性，很多人都独立的研究出了这个增长现象，直到1922年一个叫做Raymond Pearl的人口学家注意到Verhulst在1838年就已经提出了这个现象和公式，并在他的文章中也使用了logistic function来称呼它，并且沿用至今。

在1920年Pearl[3]在研究美国人口增长规律时提出了另外一种表示logistic function的方法。

$y = \dfrac{be^{ax}}{1+ce^{ax}}$ ，基于这个表达式，Joseph Berkson在1944年提出了logit function，$logit=In(\dfrac{1−Q}{Q})$，假如 $Q=\dfrac{1}{1+e^{a−bx}}$，结果就是$logit=a−bx$。

在1958年David Cox提出了logistic regression，是为了解决：有一组取值为0，1的观测值，它们的取值Yi依赖于一些独立变量xi， 当Yi=1时对应的概率为θi=pr(Yi=1)。由于θi限制在[0,1]之间，因此假设θi与xi的关系符合logit function，即$logit\theta_i \equiv log{\dfrac{\theta_i}{1-\theta_i}} = \alpha + \beta x_i$,

Cramer在他的文章中有更加详细的讨论。它是由数学家对人口发展规律研究得出，后来又被应用到了微生物生长情况的研究，后来又被应用解决经济学相关问题，直到发展到今天作为一个非常重要的算法而存在于各行各业。

### 模型介绍与公式推导

#### Logistic Distribution

随机变量X服从逻辑斯蒂分布，即X的累积分布函数为上文提到过的logistic function。对分布函数求导得到了概率密度函数。

$F(x) = P(X \leqslant x) = \dfrac{1}{1+e^{-(x-\mu)/\gamma}}$

$f(x) = F’(x) = \dfrac{e^{-(x-\mu)/\gamma}} { \gamma (1+e^{-(x-\mu)/\gamma})^2 }$

![20180419lr01](https://github.com/appletrue/NoteML/blob/master/PICs/20180419lr01.png)

μ影响的是中心对称点的位置，γ越小中心点附近增长的速度越快。常在深度学习中用到非线性变换sigmoid函数是逻辑斯蒂分布的γ=1,μ=0的特殊形式。

#### 二分类问题

**逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。**

如：**用户：购买频次，浏览频次，时间，地理位置...**

**品类：销量，购买用户，浏览用户...**

**交叉：购买频次，浏览频次，购买间隔...**

逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。回归是一种极易理解的模型，就相当于y=f(x)，表明自变量x与因变量y的关系。最常见问题有如医生治病时的望、闻、问、切，之后判定病人是否生病或生了什么病，其中的望闻问切就是获取自变量x，即特征数据，判断是否生病就相当于获取因变量y，即预测分类。

![20180419lr02](https://github.com/appletrue/NoteML/blob/master/PICs/20180419lr02.png)

目标是判断圆圈属于哪一类。也就是说逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。假设已经存在这样一个边界，针对于图中这种线性可分的情况，这条边界是输入特征向量的线性组合，假设输入的特征向量为$x\in R^n$(图中输入向量为二维)，Y取值为0，1。那么决策边界可以表示为$w1x1+w2x2+b=0$，假如存在一个例子使得$hw(x)=w1x1+w2x2+b>0$，那么可以判断它类别为1，这个过程实际上是感知机，即只通过决策函数的符号来判断属于哪一类。而逻辑回归需要再进一步，它要找到分类概率P(Y=1)与输入向量x的直接关系，然后通过比较概率值来判断类别，令决策函数的输出值$w^Tx+b$等于概率值比值取对数$log{\dfrac{P(Y=1|x)}{1-P(Y=1|x)}}$，求解这个式子得到了输入向量x下导致产生两类的概率为:

$P(Y=1|x)=\dfrac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(1)$

$P(Y=0|x)=\dfrac{1}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(2)$ 

其中w称为权重，b称为偏置，其中的w⋅x+b看成对x的线性函数。对比上面两个概率值，概率值大的就是x对应的类。

已知一个事件发生的几率odds是指该事件发生与不发生的概率比值，二分类情况下即$\frac {P(Y=1|x)}{P(Y=0|x)} = \frac {P(Y=1|x)}{1-P(Y=1|x)}$。

取odds的对数就是上面提到的logit function，

$logit(P(Y=1|x)) = log\dfrac {P(Y=1|x)}{1-P(Y=1|x)} = w\cdot x$

从而可以得到一种对逻辑回归的定义，**输出Y=1的对数几率是由输入xx的线性函数表示的模型，即逻辑斯蒂回归模型**。

公式1可以得到另一种对逻辑回归的定义，**线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型**

因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。

在推导多分类的问题时，是假设$w_1^Tx + b_1 = \dfrac{P(Y=1|x)}{P(Y=K|x)}$,$w_2^Tx + b_2 = \dfrac{P(Y=2|x)}{P(Y=K|x)}$等，再推导出$P(Y=K|x) = \dfrac{1}{1+\sum_{k=1}^{K-1}e^{w_k^Tx}}$, $(Y=1|x) = \dfrac{e^{w_1^Tx}}{1+\sum_{k=1}^{K-1}e^{w_k^Tx}}$等。

有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$P(Y=1|x)=h_w (x)$,似然函数为$\prod [h_w(x_i)]^{y_i}[1-h_w(x_i)]^{(1-y_i)}$,对数似然函数为 

$L(w) = \sum _{i=1}^{N}logP(y_i|x_i;w) = \sum_{i=1}^{N}[y_ilog h_w (x_i) +(1-y_i)log(1-h_w(x_i))] \:\:\:\:\:\:\:\:\:(3)$

### 解法

只介绍梯度下降，牛顿法。优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是

$min J(w) = min {-\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))]} \:\:\:\:\:\:\:\:\:(4)$

先把J(w)对wj的一阶二阶偏导求出来，且分别用g和H表示。g是梯度向量，H是海森矩阵。这里只考虑一个实例yi产生的似然函数对一个参数wj的偏导。

$g_j = \dfrac{\partial J(w)} {\partial w_j} = \dfrac{y^{(i)}}{h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))(-x_{j}^{(i)})+(1-y^{(i)})\dfrac {1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}=(y^{(i)}-h_w(x^{(i)}))x^{(i)}   \:\:\:\:\:\:\:\:\:(5)$

$H_{mn} = \dfrac {\partial^2 J(w)} {\partial w_m \partial w_n} =h_w(x^{(i)})(1-h_w(x^{(i)}))x^{(i)}_mx^{(i)}_n \:\:\:\:\:\:\:\:\:(6)$

这几种方法一般都是采用迭代的方式来逐步逼近极小值，需要给定参数w0w0作为起点，并且需要一个阈值ϵϵ来判断迭代何时停止。

#### 1. 梯度下降法

梯度下降是通过J(w)对w的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为$w_j^{k+1} = w_j^k + \alpha g_j$，k为迭代次数。每次更新参数后，可以通过比较$||J(w^{k+1})-J(w^k)||$或者$||w^{k+1}-w^k||$与某个阈值ϵϵ大小的方式来停止迭代，即比阈值小就停止。

#### 2. 牛顿法

牛顿法的基本思路是，**在现有极小点估计值的附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值**。假设wk为当前的极小值估计值，那么有

$\varphi (w) = J(w^k) + J’(w^k)(w-w^k)+\frac{1}{2}J’’(w^k)(w-w^k)^2  \:\:\:\:\:\:\:\:\:(7)$

然后令φ′(w)=0，得到了$w=w^k-\frac{J’(w^k)}{J’’(w^k)}$。因此有迭代更新式，

$w^{k+1} = w^k - \frac{J’(w^k)}{J’’(w^k)} = w^k - H_k^{-1}\cdot g_k \:\:\:\:\:\:\:\:\:(8)$

此方法中也需要一个阈值ϵ，当时停止迭代。此外，这个方法需要目标函数是二阶连续可微的，本文中的J(w)是符合要求的。

### 模型

问题可以简化为，如何找到这样一个决策函数y∗=f(x)，它在未知数据集上能有足够好的表现。

至于如何衡量一个二分类模型的好坏，我们可以用分类错误率这样的指标：$Err = \frac{1}{N} \sum 1[y^* = y]$ 。也可以用准确率，召回率，AUC等指标来衡量。

#### Sigmoid函数

引入sigmoid函数，sigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0/1。这个性质使我们能够以概率的方式来解释。其数学形式是：$g(x) = \frac{1}{1 + e ^ {-x}}$

#### 决策函数

一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：

$P(y=1|x;\theta) = g(\theta^T x) = \frac{1}{1 + e ^ {-\theta^T * x}}$

这里的 g(h)g(h) 是上边提到的 sigmoid 函数，相应的决策函数为：

$y^* = 1, \, \textrm{if} \, P(y=1|x) > 0.5$

选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。

#### 参数求解

模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大。在逻辑回归模型中，似然度可表示为：

$L(\theta) = P(D|\theta) = \prod P(y|x;\theta) = \prod g(\theta^T x) ^ y (1-g(\theta^T x))^{1-y}$

取对数可以得到对数似然度：

$l(\theta) = \sum {y\log{g(\theta^T x)} + (1-y)\log{(1-g(\theta^T x))}}$

另一方面，在机器学习领域，我们更经常遇到的是损失函数的概念，其衡量的是模型预测错误的程度。常用的损失函数有0-1损失，log损失，hinge损失等。其中log损失在单个数据点上的定义为

$-y\log{p(y|x)}-(1-y)\log{1-p(y|x)}$

如果取整个数据集上的平均log损失，我们可以得到

$J(\theta) = -\frac{1}{N} l(\theta)$

即在逻辑回归模型中，我们最大化似然函数和最小化log损失函数实际上是等价的。对于该优化问题，存在多种求解方法，这里以梯度下降的为例说明。梯度下降(Gradient Descent)又叫作最速梯度下降，是一种迭代求解的方法，通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。基本步骤如下：

- 选择下降方向（梯度方向，∇J(θ)）
- 选择步长，更新参数$\theta^i = \theta^{i-1} - \alpha^i \nabla {J(\theta^{i-1})}$
- 重复以上两步直到满足终止条件

![20180419gradi_descent](https://github.com/appletrue/NoteML/blob/master/PICs/20180419gradi_descent.png)

其中损失函数的梯度计算方法为：

$\frac{\partial{J}}{\partial{\theta}} = -\frac{1}{n}\sum_i (y_i - y_i^*)x_i + \lambda \theta$

沿梯度负方向选择一个较小的步长可以保证损失函数是减小的，另一方面，逻辑回归的损失函数是凸函数（加入正则项后是严格凸函数），可以保证我们找到的局部最优值同时是全局最优。此外，常用的凸优化的方法都可以用于求解该问题。例如共轭梯度下降，牛顿法，LBFGS等。

#### 分类边界

知道如何求解参数后，我们来看一下模型得到的最后结果是什么样的。很容易可以从sigmoid函数看出，当$θ^Tx>0$ 时，y=1，否则 y=0。$θ^Tx=0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性分类曲线（通过特征映射）。

![20180419decision_boundary_1](https://github.com/appletrue/NoteML/blob/master/PICs/20180419decision_boundary_1.png)

![20180419gradi_descent](https://github.com/appletrue/NoteML/blob/master/PICs/20180419gradi_descent.png)

左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换 $[x_1, x_2] => [x_1, x_2, x_1^2, x_2^2, x_1x_2]$ 后的空间是线性可分的，对应的原始空间中分类边界为一条类椭圆曲线。

#### 正则化

正则化不是只有逻辑回归存在，它是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合。

当模型的参数过多时，很容易遇到过拟合的问题。这时就需要有一种方法来控制模型的复杂度，典型的做法在优化目标中加入正则项，通过惩罚过大的参数来防止过拟合：

$J(\theta) = -\frac{1}{N}\sum {y\log{g(\theta^T x)} + (1-y)\log{(1-g(\theta^T x))}} + \lambda \Vert w \Vert_p$

一般情况下，取p=1或p=2，分别对应L1，L2正则化，两者的区别可以从下图中看出来，L1正则化（左图）倾向于使参数变为0，因此能产生稀疏解。

![20180419lr01](https://github.com/appletrue/NoteML/blob/master/PICs/20180419lr01.png)

![20180419lr02](https://github.com/appletrue/NoteML/blob/master/PICs/20180419lr02.png)

实际应用时，由于我们数据的维度可能非常高，L1正则化因为能产生稀疏解，使用的更为广泛一些。

### 生成模型和判别模型

逻辑回归是一种判别模型，表现为直接对条件概率P(y|x)建模，而不关心背后的数据分布P(x,y)。而高斯贝叶斯模型（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算样本属于各个类别的后验概率，即：

$p(y|x) = \frac{P(x|y)P(y)}{\sum{P(x|y)P(y)}}$

通常假设P(x|y)是高斯分布，P(y)是多项式分布，相应的参数都可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到：

$\log\frac{P(y=1|x)}{P(y=0|x)} = \log\frac{P(x|y=1)}{P(x|y=0)} + \log\frac{P(y=1)}{P(y=0)}  \ = -\frac{(x-\mu_1)^2}{2\sigma_1^2} + \frac{(x-\mu_0)^2}{2\sigma_0^2}\ + \theta_0$

如果 σ1=σ0，二次项会抵消，我们得到一个简单的线性关系：

$\log\frac{P(y=1|x)}{P(y=0|x)} = \theta^T x$

$P(y=1|x) = \frac{e^{\theta^T x}}{1+e^{\theta^T x}} = \frac{1}{1+e^{-\theta^T x}}$

可以看到，这个概率和逻辑回归中的形式是一样的。这种情况下GNB 和 LR 会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。

### 多分类（softmax)

如果yy不是在[0,1]中取值，而是在KK个类别中取值，这时问题就变为一个多分类问题。有两种方式可以出处理该类问题：一种是我们对每个类别训练一个二元分类器（One-vs-all），当KK个类别不是互斥的时候，比如用户会购买哪种品类，这种方法是合适的。如果KK个类别是互斥的，即 y=iy=i 的时候意味着 yy 不能取其他的值，比如用户的年龄段，这种情况下 Softmax 回归更合适一些。Softmax 回归是直接对逻辑回归在多分类的推广，相应的模型也可以叫做多元逻辑回归（Multinomial Logistic Regression）。模型通过 softmax 函数来对概率建模，具体形式如下：

$P(y=i|x, \theta) = \frac{e^{\theta_i^T x}}{\sum_j^K{e^{\theta_j^T x}}}$

而决策函数为：$y^∗=argmax_iP(y=i|x,θ)$

对应的损失函数为：

$J(\theta) = -\frac{1}{N} \sum_i^N \sum_j^K {1[y_i=j] \log{\frac{e^{\theta_i^T x}}{\sum {e^{\theta_k^T x}}}}}$


-------------------------------***参考网址***---------------------------------

[文墨](https://www.cnblogs.com/sparkwen/p/3441197.html)

[浅析Logistic Regression--一点都不浅析呢](https://chenrudan.github.io/blog/2016/01/09/logisticregression.html)

[meituan逻辑回归介绍](https://tech.meituan.com/intro_to_logistic_regression.html)
